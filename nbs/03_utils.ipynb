{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1033f25c-4f71-4c26-9893-75f2ff76b5c6",
   "metadata": {},
   "source": [
    "# Dragon Utilities\n",
    "\n",
    "This notebook contains all the utility functions required for diffusing the dragon. If you're looking for the 'final' Dragon class that wraps all the functionality, that'll be in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427953dd-a276-4511-9820-83ad1e39ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899e8fc-01e0-47d1-91c7-dcaefd785334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import lr_scheduler\n",
    "from diffusers import UNet2DModel\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from diffusers import LMSDiscreteScheduler, UNet2DConditionModel, AutoencoderKL, DDIMScheduler\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from diffusers.utils import BaseOutput\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput, deprecate\n",
    "from diffusers.schedulers.scheduling_utils import SchedulerMixin\n",
    "\n",
    "from dragon_diffusion.core import *\n",
    "from dragon_diffusion.masks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451eb4a-4b1d-46de-9db9-0d129f706417",
   "metadata": {},
   "source": [
    "### Feature hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64203df-b7a8-4421-8d8f-3bc4cf69ef50",
   "metadata": {},
   "source": [
    "These functions collect features from a specific layer in the model for each forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e7efb-6e2e-4ef1-938b-6557803fe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_features(hook, layer, inp, out):\n",
    "    if not hasattr(hook, 'feats'): hook.feats = out\n",
    "    hook.feats = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d138f51-fe61-422c-b7ff-753c3e4a12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hook():\n",
    "    def __init__(self, model, func): self.hook = model.register_forward_hook(partial(func, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add5f6e-503a-41d5-8784-239e12484579",
   "metadata": {},
   "source": [
    "### Attention control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3c056-6125-4294-b14b-f10a815c0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from diffusers.models.attention_processor import AttnProcessor, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04c997-0a58-47b5-84d0-0d5ed6779ac9",
   "metadata": {},
   "source": [
    "The function below will take a custom attention processor, and inject into a specific section of the model. The paper discusses using attention control 'only in the decoder', and using the `location` parameter enables this level of dexterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6839d1b-e92e-49d9-a823-67f7df9e7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_attn_dict(processor, model, attention_type='all', location='all'):\n",
    "    \"\"\"\n",
    "        Enables selective replacement of attention processors. For DragonDiffusion \n",
    "        use attention_type='attn1' (self-attention blocks) and location='decoder'. Looks\n",
    "        overly complicated but just enables fine-grained control of where in the model\n",
    "        the attention processors are replaced.\n",
    "    \"\"\"\n",
    "    assert attention_type in ['all', 'attn1', 'attn2']\n",
    "    assert location in ['all', 'decoder']\n",
    "    attn_procs = {}\n",
    "    for name in model.attn_processors.keys():\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "    if attention_type == 'all' and location == 'all':\n",
    "        for key in attn_procs.keys(): attn_procs[key] = processor(name=key)\n",
    "    elif attention_type == 'attn1' and location == 'decoder':\n",
    "        keys = [k for k in attn_procs.keys() if 'up_blocks' in k and 'attn1' in k]\n",
    "        for key in keys: attn_procs[key] = processor(name=key)\n",
    "    elif attention_type == 'attn1' and location == 'all':\n",
    "        keys = [k for k in attn_procs.keys() if 'attn1' in k]\n",
    "        for key in keys: attn_procs[key] = processor(name=key)\n",
    "    elif attention_type == 'attn2' and location == 'decoder':\n",
    "        keys = [k for k in attn_procs.keys() if 'up_blocks' in k and 'attn2' in k]\n",
    "        for key in keys: attn_procs[key] = processor(name=key)\n",
    "    elif attention_type == 'attn2' and location == 'all':\n",
    "        keys = [k for k in attn_procs.keys() if 'attn2' in k]\n",
    "        for key in keys: attn_procs[key] = processor(name=key)\n",
    "    elif attention_type == 'all' and location == 'decoder':\n",
    "        keys = [k for k in attn_procs.keys() if 'up_blocks' in k]\n",
    "        for key in keys: attn_procs[key] = processor(name=key)\n",
    "    return attn_procs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7715c14-38f0-4c71-b588-3c71fbca878a",
   "metadata": {},
   "source": [
    "The function below is a simple storage object for attention. Specifically, it stores the `key` and `value` matrices from the forward pass of the diffusion branch concerned with reconstructing the original image, so that this can be injected into the editing branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353a093-36d6-40d8-8b08-59ab45de01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AttnStorage:\n",
    "    def __init__(self): self.storage = {}\n",
    "    def __call__(self, name, key, value, emb_type):\n",
    "        if not emb_type in self.storage: self.storage[emb_type] = {}\n",
    "        if not name in self.storage[emb_type]: self.storage[emb_type][name] = {}\n",
    "        self.storage[emb_type][name]['key'] = key\n",
    "        self.storage[emb_type][name]['value'] = value\n",
    "    def flush(self): self.storage = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12362228-3b97-4b97-b490-d9ae282b2c09",
   "metadata": {},
   "source": [
    "Below is a custom attention processor. It doesn't do anything fancy, it just takes the `key` and `value` matrices from the forward passes and commits them to storage in the `attn_storage` object (above). It also enables injection of key and value matrices from the storage object. This is how we will do attention control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08dd3b-dd31-4ef1-9e8a-aeab3c36fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomAttnProcessor(AttnProcessor):\n",
    "    def __init__(self, attn_storage, name=None): \n",
    "        fc.store_attr()\n",
    "        self.store_attention = False\n",
    "        self.inject_attention = False\n",
    "    def set_attention(self, store, inject, emb_type='cond'): \n",
    "        self.store_attention = store\n",
    "        self.emb_type = emb_type\n",
    "        self.inject_attention = inject\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "     \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        if self.store_attention: self.attn_storage(self.name, key, value, self.emb_type) ## store key and value matrices\n",
    "        \n",
    "        if self.inject_attention: ## inject corresponding key and value matrices\n",
    "            key = self.attn_storage.storage[self.emb_type][self.name]['key']\n",
    "            value = self.attn_storage.storage[self.emb_type][self.name]['value']\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        attention_probs.requires_grad_(True)\n",
    "        \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473a0fb-72a7-4880-8ece-ad21d6f97620",
   "metadata": {},
   "source": [
    "Finally, the function below just makes it easier to flick the switch for the attention processor between normal, storage and injection modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9b7f7-49ee-4b43-bb79-0c8d2c0b7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_attention(model, attn_storage, set_store=False, set_inject=False, emb_type='cond'):\n",
    "    assert emb_type in ['cond', 'uncond']\n",
    "    assert not (set_store is True and set_inject is True)\n",
    "    for name, module in model.attn_processors.items(): \n",
    "        if \"CustomAttnProcessor\" in module.__class__.__name__: module.set_attention(set_store, set_inject, emb_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc583e-1914-4aa3-8c89-1b76ebc9b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4a7ce-38a9-46a3-9751-e45118194785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
